{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b66cd-c281-44eb-a2bb-e31aa41347db",
   "metadata": {},
   "outputs": [],
   "source": "from huggingface_hub import login\nfrom decouple import config\n\nhuggingface_token = config('HUGGINGFACE_TOKEN')\nlogin(token=huggingface_token, add_to_git_credential=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a467055-b046-415a-8a99-004667f10ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from transformers import Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import datasets\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeaac69e-fdaa-43b4-bf13-407e5b475ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "# model_id = 'Bllossom/llama-3.2-Korean-Bllossom-AICA-5B'\n",
    "model_id = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715fba21-fd0e-4d63-bb68-64824e50b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct-AWQ\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Choose your prompt\n",
    "# # prompt = \"Explain how wonderful you are\"  # English example\n",
    "# prompt = \"스스로를 자랑해 봐\"       # Korean example\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \n",
    "#      \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ]\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=True,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# )\n",
    "\n",
    "# output = model.generate(\n",
    "#     input_ids.to(\"cuda\"),\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     max_new_tokens=128,\n",
    "#     do_sample=False,\n",
    "# )\n",
    "# print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf700d-35ec-42ea-8a89-dac224f74bc4",
   "metadata": {},
   "source": [
    "## 파인 튜닝 전 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec1135-0814-41e1-8d3c-a4968eb493bb",
   "metadata": {},
   "source": [
    "## 4비트 양자화 설정(QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b7579d-613c-44d0-835d-4147fe80a8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Attention 2를 사용합니다 (bfloat16).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd50b40b6004806b7138ac4b208eabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bllossom-3B\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "    print(\"Flash Attention 2를 사용합니다 (bfloat16).\")\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "    print(\"Eager Attention을 사용합니다 (float16).\")\n",
    "\n",
    "# QLoRA config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "config.attention_implementation = attn_implementation \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4583106-239a-4544-b0ff-52d34155afc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExaoneForCausalLM(\n",
       "  (transformer): ExaoneModel(\n",
       "    (wte): Embedding(102400, 2560, padding_idx=0)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-29): 30 x ExaoneBlock(\n",
       "        (ln_1): ExaoneRMSNorm()\n",
       "        (attn): ExaoneAttention(\n",
       "          (attention): ExaoneSdpaAttention(\n",
       "            (rotary): ExaoneRotaryEmbedding()\n",
       "            (k_proj): Linear4bit(in_features=2560, out_features=640, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=2560, out_features=640, bias=False)\n",
       "            (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=False)\n",
       "            (out_proj): Linear4bit(in_features=2560, out_features=2560, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): ExaoneRMSNorm()\n",
       "        (mlp): ExaoneGatedMLP(\n",
       "          (c_fc_0): Linear4bit(in_features=2560, out_features=7168, bias=False)\n",
       "          (c_fc_1): Linear4bit(in_features=2560, out_features=7168, bias=False)\n",
       "          (c_proj): Linear4bit(in_features=7168, out_features=2560, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): ExaoneRMSNorm()\n",
       "    (rotary): ExaoneRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e670ea68-7e2b-4d74-8807-4b8d86a299d1",
   "metadata": {},
   "source": [
    "## Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87db5233-6a5e-4c63-84ce-4134261b22c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 생성 완료\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 6115\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "from datasets import Dataset\n",
    "\n",
    "# JSONLines 파일 경로\n",
    "jsonl_path = '/mnt/ssd/1/sanguk/dataset/iljoo_expanded_responses_dataset.jsonl'\n",
    "\n",
    "# JSONLines 파일을 읽어서 데이터셋 생성\n",
    "indataset = []\n",
    "with jsonlines.open(jsonl_path) as f:\n",
    "    for lineno, line in enumerate(f.iter(), start=1):\n",
    "        try:\n",
    "            # Q&A 형태 템플릿으로 instruction과 response 형식을 맞춰서 저장\n",
    "            template = \"{instruction}\\n{response}\"\n",
    "            indataset.append(template.format(**line))\n",
    "        except Exception as e:\n",
    "            # 문제가 있는 줄과 오류를 출력하여 확인\n",
    "            print(f\"Error at line {lineno}: {e}\")\n",
    "\n",
    "# 데이터셋 확인\n",
    "print('데이터셋 생성 완료')\n",
    "\n",
    "# Hugging Face Dataset으로 변환\n",
    "indataset = Dataset.from_dict({'text': indataset})\n",
    "\n",
    "# 데이터셋 정보 확인\n",
    "print(indataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48c336e-64e0-4bb9-9eac-cd3999f6b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # 패딩 토큰을 시퀀스 어느 쪽에 할지\n",
    "\n",
    "def preprocess_dataset(example):\n",
    "    tokenized = tokenizer(example[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f90a20bf-f198-4af7-b364-638ddea07ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b763d6a1c1bb4f66baf52d9cef9192f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/4892 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2c79d4ef71485da8e08ed475a2512f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 4892\n",
      "})\n",
      "Train dataset length: 4892\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1223\n",
      "})\n",
      "Valid dataset length: 1223\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# train_test_split으로 분할 (반환되는 키는 \"train\"과 \"test\")\n",
    "train_valid_split = indataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# 새로운 DatasetDict 객체를 생성하여 \"test\" 키 대신 \"validation\" 키로 재할당\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_valid_split[\"train\"],\n",
    "    \"validation\": train_valid_split[\"test\"]\n",
    "})\n",
    "\n",
    "# 이후 전처리 진행 (이 때 validation 데이터셋으로 사용)\n",
    "train_dataset = datasets[\"train\"].map(\n",
    "    preprocess_dataset,\n",
    "    num_proc=2,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "valid_dataset = datasets[\"validation\"].map(\n",
    "    preprocess_dataset,\n",
    "    num_proc=2,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "print(train_dataset)\n",
    "print(\"Train dataset length:\", len(train_dataset))\n",
    "print(valid_dataset)\n",
    "print(\"Valid dataset length:\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "388ff78d-387a-4546-bf56-6bccd136e18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96064, 27458, 41728, 76007, 61016, 696, 3157, 1130, 657, 869]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"input_ids\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c071a25-7318-47c3-b6a8-53645f15bcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TeamsAI에서 coco 포맷을 지원하는 게 뭐가 중요한가요?\\nTeamsAI가 coco 포맷을 지원하면 데이터 이동이 더 쉬워져요. 그래서 다양한 AI 시스템과 통합할 때 편리하고요.[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[0][\"input_ids\"][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367f024-b863-4493-b8b4-af89ecc1bcf4",
   "metadata": {},
   "source": [
    "## Data Collator\n",
    "\n",
    "- LLM(대형 언어 모델) 파인튜닝 시 **Data Collator**는 배치(batch) 단위로 데이터를 정리하여 모델에 전달하는 역할을 합니다.\n",
    "- 일반적으로 DataLoader와 함께 사용되며, 토큰 길이를 맞추거나, 필요한 마스크 값을 추가하는 등의 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f720b68-d061-4f3f-9455-891f1fff5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # causal LM인 경우에는 False로 설정합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb5317-c7bc-4b62-8163-4e8e34e561b0",
   "metadata": {},
   "source": [
    "## Configuration TrainingArugments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2e11995-c67a-4ba6-a1da-cb6f60eadf17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "hub_path = \"/mnt/ssd/1/hub\"\n",
    "save_model_path = os.path.join(hub_path, \"models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1\")\n",
    "\n",
    "class EarlyStoppingWithCombinedLossCallback(EarlyStoppingCallback):\n",
    "    def __init__(self, weight_train=0.4, weight_eval=0.6, early_stopping_patience=20):\n",
    "        super().__init__(early_stopping_patience=early_stopping_patience)\n",
    "        self.weight_train = weight_train\n",
    "        self.weight_eval = weight_eval\n",
    "        self.last_train_loss = None\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # train loss를 저장합니다.\n",
    "        if logs is not None and \"loss\" in logs:\n",
    "            self.last_train_loss = logs[\"loss\"]\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        # eval_loss가 반드시 metrics에 존재한다고 가정합니다.\n",
    "        eval_loss = metrics.get(\"eval_loss\")\n",
    "        if eval_loss is None:\n",
    "            eval_loss = float(\"inf\")\n",
    "\n",
    "        # train loss가 아직 없다면 combined loss는 eval_loss로 대체합니다.\n",
    "        if self.last_train_loss is None:\n",
    "            combined_loss = eval_loss\n",
    "        else:\n",
    "            combined_loss = self.weight_train * self.last_train_loss + self.weight_eval * eval_loss\n",
    "\n",
    "        # 평가 metrics에 combined loss 추가\n",
    "        metrics[\"eval_combined_loss\"] = combined_loss\n",
    "\n",
    "        # 현재 global step에 따른 checkpoint 경로 생성\n",
    "        current_step = state.global_step\n",
    "        current_checkpoint = os.path.join(args.output_dir, f\"checkpoint-{current_step}\")\n",
    "\n",
    "        # best_metric이 아직 설정되지 않았거나, 지금의 combined_loss가 더 낮으면 업데이트\n",
    "        if state.best_metric is None or combined_loss < state.best_metric:\n",
    "            state.best_metric = combined_loss\n",
    "            state.best_model_checkpoint = current_checkpoint\n",
    "            print(f\"New best checkpoint: {current_checkpoint} with Combined Loss: {combined_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Combined Loss at step {current_step}: {combined_loss:.4f}\")\n",
    "\n",
    "        # 기존 EarlyStoppingCallback의 로직을 실행하여 조기 종료 여부를 체크합니다.\n",
    "        return super().on_evaluate(args, state, control, metrics, **kwargs)\n",
    "        \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_model_path,\n",
    "    per_device_train_batch_size=8,                  \n",
    "    per_device_eval_batch_size=8, \n",
    "    gradient_accumulation_steps=4,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    bf16=True,\n",
    "    num_train_epochs=100,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=50,\n",
    "    label_names=['labels'],\n",
    "    metric_for_best_model=\"eval_combined_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    eval_strategy=\"steps\", \n",
    "    eval_steps=50,\n",
    "    save_on_each_node=True,\n",
    "    save_total_limit=1,\n",
    "    torch_compile=True,\n",
    "    group_by_length=True,             \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "training_args.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7c38f-1d6c-4771-aebd-79d713dc52a2",
   "metadata": {},
   "source": [
    "## Configuration Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c53edbf-0b32-4221-a641-cbaee69f4f0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22,118,400 || all params: 2,427,445,760 || trainable%: 0.9112\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Lora Tuning\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"]\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# LoRA 파라미터만 학습하도록 설정\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if \"lora_\" in name:\n",
    "        param.requires_grad = True  \n",
    "    else:\n",
    "        param.requires_grad = False  \n",
    "        \n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db443bc3-671a-4237-8063-38f66280a64d",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375fc3b-a886-42d0-a774-e1ce3fca13fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning 시작...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2709' max='15300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2709/15300 2:30:27 < 11:39:50, 0.30 it/s, Epoch 17.70/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "      <th>Combined Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.207500</td>\n",
       "      <td>1.939771</td>\n",
       "      <td>44.254300</td>\n",
       "      <td>27.636000</td>\n",
       "      <td>3.457000</td>\n",
       "      <td>2.046863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.859200</td>\n",
       "      <td>1.784239</td>\n",
       "      <td>43.988500</td>\n",
       "      <td>27.803000</td>\n",
       "      <td>3.478000</td>\n",
       "      <td>1.814223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.729100</td>\n",
       "      <td>1.698546</td>\n",
       "      <td>43.964200</td>\n",
       "      <td>27.818000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>1.710768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.640000</td>\n",
       "      <td>1.640615</td>\n",
       "      <td>44.225700</td>\n",
       "      <td>27.654000</td>\n",
       "      <td>3.460000</td>\n",
       "      <td>1.640369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.582900</td>\n",
       "      <td>1.592080</td>\n",
       "      <td>43.987800</td>\n",
       "      <td>27.803000</td>\n",
       "      <td>3.478000</td>\n",
       "      <td>1.588408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.565000</td>\n",
       "      <td>1.548133</td>\n",
       "      <td>44.126000</td>\n",
       "      <td>27.716000</td>\n",
       "      <td>3.467000</td>\n",
       "      <td>1.554880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.476600</td>\n",
       "      <td>1.517886</td>\n",
       "      <td>44.026600</td>\n",
       "      <td>27.779000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>1.501372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.458800</td>\n",
       "      <td>1.488895</td>\n",
       "      <td>43.920900</td>\n",
       "      <td>27.845000</td>\n",
       "      <td>3.484000</td>\n",
       "      <td>1.476857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.407200</td>\n",
       "      <td>1.469602</td>\n",
       "      <td>44.036200</td>\n",
       "      <td>27.773000</td>\n",
       "      <td>3.474000</td>\n",
       "      <td>1.444641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.359400</td>\n",
       "      <td>1.449290</td>\n",
       "      <td>43.871600</td>\n",
       "      <td>27.877000</td>\n",
       "      <td>3.487000</td>\n",
       "      <td>1.413334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.340600</td>\n",
       "      <td>1.422123</td>\n",
       "      <td>44.125500</td>\n",
       "      <td>27.716000</td>\n",
       "      <td>3.467000</td>\n",
       "      <td>1.389514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.325500</td>\n",
       "      <td>1.403971</td>\n",
       "      <td>44.067900</td>\n",
       "      <td>27.753000</td>\n",
       "      <td>3.472000</td>\n",
       "      <td>1.372583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.275600</td>\n",
       "      <td>1.395184</td>\n",
       "      <td>43.976900</td>\n",
       "      <td>27.810000</td>\n",
       "      <td>3.479000</td>\n",
       "      <td>1.347351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.239000</td>\n",
       "      <td>1.375452</td>\n",
       "      <td>44.028800</td>\n",
       "      <td>27.777000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>1.320871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.233500</td>\n",
       "      <td>1.359772</td>\n",
       "      <td>43.982000</td>\n",
       "      <td>27.807000</td>\n",
       "      <td>3.479000</td>\n",
       "      <td>1.309263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.183200</td>\n",
       "      <td>1.354107</td>\n",
       "      <td>44.008900</td>\n",
       "      <td>27.790000</td>\n",
       "      <td>3.477000</td>\n",
       "      <td>1.285744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.161000</td>\n",
       "      <td>1.337708</td>\n",
       "      <td>43.884600</td>\n",
       "      <td>27.869000</td>\n",
       "      <td>3.486000</td>\n",
       "      <td>1.267025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.156700</td>\n",
       "      <td>1.322751</td>\n",
       "      <td>44.041100</td>\n",
       "      <td>27.770000</td>\n",
       "      <td>3.474000</td>\n",
       "      <td>1.256330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.109200</td>\n",
       "      <td>1.332663</td>\n",
       "      <td>43.979200</td>\n",
       "      <td>27.809000</td>\n",
       "      <td>3.479000</td>\n",
       "      <td>1.243278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.088700</td>\n",
       "      <td>1.322640</td>\n",
       "      <td>43.878700</td>\n",
       "      <td>27.872000</td>\n",
       "      <td>3.487000</td>\n",
       "      <td>1.229064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.110400</td>\n",
       "      <td>1.319234</td>\n",
       "      <td>44.095200</td>\n",
       "      <td>27.735000</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>1.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.056800</td>\n",
       "      <td>1.324922</td>\n",
       "      <td>43.909400</td>\n",
       "      <td>27.853000</td>\n",
       "      <td>3.484000</td>\n",
       "      <td>1.217673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.033000</td>\n",
       "      <td>1.326381</td>\n",
       "      <td>44.074900</td>\n",
       "      <td>27.748000</td>\n",
       "      <td>3.471000</td>\n",
       "      <td>1.209029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.022000</td>\n",
       "      <td>1.314434</td>\n",
       "      <td>44.022900</td>\n",
       "      <td>27.781000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>1.197461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.992400</td>\n",
       "      <td>1.336939</td>\n",
       "      <td>43.965800</td>\n",
       "      <td>27.817000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>1.199124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.974800</td>\n",
       "      <td>1.331095</td>\n",
       "      <td>44.064600</td>\n",
       "      <td>27.755000</td>\n",
       "      <td>3.472000</td>\n",
       "      <td>1.188577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>1.311792</td>\n",
       "      <td>43.931200</td>\n",
       "      <td>27.839000</td>\n",
       "      <td>3.483000</td>\n",
       "      <td>1.176675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.943500</td>\n",
       "      <td>1.335603</td>\n",
       "      <td>43.984500</td>\n",
       "      <td>27.805000</td>\n",
       "      <td>3.479000</td>\n",
       "      <td>1.178762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.902200</td>\n",
       "      <td>1.342075</td>\n",
       "      <td>43.978100</td>\n",
       "      <td>27.809000</td>\n",
       "      <td>3.479000</td>\n",
       "      <td>1.166125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.928500</td>\n",
       "      <td>1.322684</td>\n",
       "      <td>44.023300</td>\n",
       "      <td>27.781000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>1.165010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>1.363428</td>\n",
       "      <td>44.235200</td>\n",
       "      <td>27.648000</td>\n",
       "      <td>3.459000</td>\n",
       "      <td>1.179657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.855100</td>\n",
       "      <td>1.358537</td>\n",
       "      <td>44.052100</td>\n",
       "      <td>27.763000</td>\n",
       "      <td>3.473000</td>\n",
       "      <td>1.157162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.872900</td>\n",
       "      <td>1.343641</td>\n",
       "      <td>44.058700</td>\n",
       "      <td>27.758000</td>\n",
       "      <td>3.473000</td>\n",
       "      <td>1.155345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.850100</td>\n",
       "      <td>1.373083</td>\n",
       "      <td>44.054800</td>\n",
       "      <td>27.761000</td>\n",
       "      <td>3.473000</td>\n",
       "      <td>1.163890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.804200</td>\n",
       "      <td>1.371776</td>\n",
       "      <td>44.084200</td>\n",
       "      <td>27.742000</td>\n",
       "      <td>3.471000</td>\n",
       "      <td>1.144745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.829100</td>\n",
       "      <td>1.370082</td>\n",
       "      <td>44.130800</td>\n",
       "      <td>27.713000</td>\n",
       "      <td>3.467000</td>\n",
       "      <td>1.153689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.798900</td>\n",
       "      <td>1.402282</td>\n",
       "      <td>44.029200</td>\n",
       "      <td>27.777000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>1.160929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.767200</td>\n",
       "      <td>1.388649</td>\n",
       "      <td>44.150300</td>\n",
       "      <td>27.701000</td>\n",
       "      <td>3.465000</td>\n",
       "      <td>1.140069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.774600</td>\n",
       "      <td>1.384144</td>\n",
       "      <td>44.034200</td>\n",
       "      <td>27.774000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>1.140326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.758400</td>\n",
       "      <td>1.444277</td>\n",
       "      <td>43.996400</td>\n",
       "      <td>27.798000</td>\n",
       "      <td>3.478000</td>\n",
       "      <td>1.169926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.703200</td>\n",
       "      <td>1.449319</td>\n",
       "      <td>44.032900</td>\n",
       "      <td>27.775000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>1.150871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.732100</td>\n",
       "      <td>1.428015</td>\n",
       "      <td>44.053800</td>\n",
       "      <td>27.761000</td>\n",
       "      <td>3.473000</td>\n",
       "      <td>1.149649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.715200</td>\n",
       "      <td>1.505986</td>\n",
       "      <td>44.023000</td>\n",
       "      <td>27.781000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>1.189672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.665900</td>\n",
       "      <td>1.493490</td>\n",
       "      <td>43.998400</td>\n",
       "      <td>27.796000</td>\n",
       "      <td>3.477000</td>\n",
       "      <td>1.162454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>1.477280</td>\n",
       "      <td>44.022100</td>\n",
       "      <td>27.782000</td>\n",
       "      <td>3.476000</td>\n",
       "      <td>1.157768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.684700</td>\n",
       "      <td>1.535307</td>\n",
       "      <td>44.038400</td>\n",
       "      <td>27.771000</td>\n",
       "      <td>3.474000</td>\n",
       "      <td>1.195064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.619100</td>\n",
       "      <td>1.548570</td>\n",
       "      <td>44.070600</td>\n",
       "      <td>27.751000</td>\n",
       "      <td>3.472000</td>\n",
       "      <td>1.176782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>1.526150</td>\n",
       "      <td>44.146600</td>\n",
       "      <td>27.703000</td>\n",
       "      <td>3.466000</td>\n",
       "      <td>1.170490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.652200</td>\n",
       "      <td>1.498206</td>\n",
       "      <td>43.923600</td>\n",
       "      <td>27.844000</td>\n",
       "      <td>3.483000</td>\n",
       "      <td>1.159804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.578100</td>\n",
       "      <td>1.579099</td>\n",
       "      <td>44.040500</td>\n",
       "      <td>27.770000</td>\n",
       "      <td>3.474000</td>\n",
       "      <td>1.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>1.553611</td>\n",
       "      <td>44.002500</td>\n",
       "      <td>27.794000</td>\n",
       "      <td>3.477000</td>\n",
       "      <td>1.173687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.609100</td>\n",
       "      <td>1.546215</td>\n",
       "      <td>44.083700</td>\n",
       "      <td>27.743000</td>\n",
       "      <td>3.471000</td>\n",
       "      <td>1.171369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.542600</td>\n",
       "      <td>1.608593</td>\n",
       "      <td>44.087000</td>\n",
       "      <td>27.741000</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>1.182196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>1.604500</td>\n",
       "      <td>44.134600</td>\n",
       "      <td>27.711000</td>\n",
       "      <td>3.467000</td>\n",
       "      <td>1.187100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-50 with Combined Loss: 2.0469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-100 with Combined Loss: 1.8142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-150 with Combined Loss: 1.7108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-200 with Combined Loss: 1.6404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-250 with Combined Loss: 1.5884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-300 with Combined Loss: 1.5549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-350 with Combined Loss: 1.5014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-400 with Combined Loss: 1.4769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-450 with Combined Loss: 1.4446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-500 with Combined Loss: 1.4133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-550 with Combined Loss: 1.3895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-600 with Combined Loss: 1.3726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-650 with Combined Loss: 1.3474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-700 with Combined Loss: 1.3209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-750 with Combined Loss: 1.3093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-800 with Combined Loss: 1.2857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-850 with Combined Loss: 1.2670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-900 with Combined Loss: 1.2563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-950 with Combined Loss: 1.2433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1000 with Combined Loss: 1.2291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 1050: 1.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1100 with Combined Loss: 1.2177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1150 with Combined Loss: 1.2090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1200 with Combined Loss: 1.1975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 1250: 1.1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1300 with Combined Loss: 1.1886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1350 with Combined Loss: 1.1767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 1400: 1.1788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1450 with Combined Loss: 1.1661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1500 with Combined Loss: 1.1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 1550: 1.1797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1600 with Combined Loss: 1.1572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1650 with Combined Loss: 1.1553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 1700: 1.1639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1750 with Combined Loss: 1.1447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 1800: 1.1537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 1850: 1.1609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best checkpoint: /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-2.4B_bf16_lr64_qlr4_test1/checkpoint-1900 with Combined Loss: 1.1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 1950: 1.1403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2000: 1.1699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2050: 1.1509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2100: 1.1496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2150: 1.1897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2200: 1.1625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2250: 1.1578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2300: 1.1951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2350: 1.1768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2400: 1.1705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2450: 1.1598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2500: 1.1787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2550: 1.1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2600: 1.1714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2650: 1.1822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss at step 2700: 1.1871\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch._dynamo\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Setting `save_embedding_layers` to `True`\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not find a config file in\")\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.cache_size_limit = 1000\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[EarlyStoppingWithCombinedLossCallback(weight_train=0.4, weight_eval=0.6, early_stopping_patience=1000)]\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning 시작...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}