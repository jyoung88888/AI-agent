{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24897dce-b7a8-4dd7-b77f-7302f17018ef",
   "metadata": {},
   "outputs": [],
   "source": "from huggingface_hub import login\nfrom decouple import config\n\nhuggingface_token = config('HUGGINGFACE_TOKEN')\nlogin(token=huggingface_token, add_to_git_credential=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708fb755-ae40-4b9b-b47b-39e6419bf84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daba2399ed6468e9c2a238773233f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 92,274,688 || all params: 1,141,511,168 || trainable%: 8.0836\n",
      "None\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): ExaoneForCausalLM(\n",
      "      (transformer): ExaoneModel(\n",
      "        (wte): Embedding(102400, 5120, padding_idx=0)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-63): 64 x ExaoneBlock(\n",
      "            (ln_1): ExaoneRMSNorm()\n",
      "            (attn): ExaoneAttention(\n",
      "              (attention): ExaoneSdpaAttention(\n",
      "                (rotary): ExaoneRotaryEmbedding()\n",
      "                (k_proj): lora.AwqLoraLinear(\n",
      "                  (base_layer): WQLinear_GEMM(in_features=5120, out_features=1024, bias=False, w_bit=4, group_size=128)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=5120, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                  (quant_linear_module): WQLinear_GEMM(in_features=5120, out_features=1024, bias=False, w_bit=4, group_size=128)\n",
      "                )\n",
      "                (v_proj): lora.AwqLoraLinear(\n",
      "                  (base_layer): WQLinear_GEMM(in_features=5120, out_features=1024, bias=False, w_bit=4, group_size=128)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=5120, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                  (quant_linear_module): WQLinear_GEMM(in_features=5120, out_features=1024, bias=False, w_bit=4, group_size=128)\n",
      "                )\n",
      "                (q_proj): lora.AwqLoraLinear(\n",
      "                  (base_layer): WQLinear_GEMM(in_features=5120, out_features=5120, bias=False, w_bit=4, group_size=128)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=5120, out_features=64, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=64, out_features=5120, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                  (quant_linear_module): WQLinear_GEMM(in_features=5120, out_features=5120, bias=False, w_bit=4, group_size=128)\n",
      "                )\n",
      "                (out_proj): WQLinear_GEMM(in_features=5120, out_features=5120, bias=False, w_bit=4, group_size=128)\n",
      "              )\n",
      "            )\n",
      "            (ln_2): ExaoneRMSNorm()\n",
      "            (mlp): ExaoneGatedMLP(\n",
      "              (c_fc_0): WQLinear_GEMM(in_features=5120, out_features=27392, bias=False, w_bit=4, group_size=128)\n",
      "              (c_fc_1): WQLinear_GEMM(in_features=5120, out_features=27392, bias=False, w_bit=4, group_size=128)\n",
      "              (c_proj): WQLinear_GEMM(in_features=27392, out_features=5120, bias=False, w_bit=4, group_size=128)\n",
      "              (act): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): ExaoneRMSNorm()\n",
      "        (rotary): ExaoneRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=5120, out_features=102400, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_name = \"LGAI-EXAONE/EXAONE-3.5-32B-Instruct-AWQ\"\n",
    "model_orin = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# LoRA 설정\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"]\n",
    ")\n",
    "\n",
    "# 모델에 LoRA 어댑터 추가\n",
    "model = get_peft_model(model_orin, peft_config)\n",
    "\n",
    "print(model.print_trainable_parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42dd345-14b8-4ac3-9fa1-6b87936b5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"/mnt/ssd/1/sanguk/dataset/iljoo_expanded_responses_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e90053a-5561-4fdb-b702-9da849fa4b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋 크기: 4892\n",
      "평가 데이터셋 크기: 1223\n"
     ]
    }
   ],
   "source": [
    "def format_sample(sample):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": sample[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": sample[\"response\"]}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, max_length=1024)\n",
    "\n",
    "# 데이터셋에 \"text\" 컬럼 추가\n",
    "dataset = dataset.map(lambda x: {\"text\": format_sample(x)})\n",
    "\n",
    "# 데이터셋을 8:2로 분할 (train: 80%, test: 20%)\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "\n",
    "# 분할된 데이터셋 확인\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# 데이터셋 크기 출력 (확인용)\n",
    "print(f\"학습 데이터셋 크기: {len(train_dataset)}\")\n",
    "print(f\"평가 데이터셋 크기: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d09ff8f-0d8d-4686-b78a-dc9df4501c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 27:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.596100</td>\n",
       "      <td>1.179814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 02:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1798144578933716, 'eval_runtime': 147.3617, 'eval_samples_per_second': 8.299, 'eval_steps_per_second': 1.038, 'epoch': 1.0}\n",
      "Merged model saved to /mnt/ssd/1/hub/models-iljoodeephub-LGAI-EXAONE-32B_AWQ_fp16_test3/checkpoint-306\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from peft import PeftModel\n",
    "\n",
    "hub_path = \"/mnt/ssd/1/hub\"\n",
    "save_model_path = os.path.join(hub_path, \"models-iljoodeephub-LGAI-EXAONE-32B_AWQ_fp16_test3\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_model_path,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    eval_strategy=\"epoch\",     \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_strategy=\"epoch\", \n",
    "    logging_first_step=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  \n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,        \n",
    "        early_stopping_threshold=0.01     \n",
    "    )],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 평가\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "# 최적 체크포인트 경로를 기준으로 병합 및 저장\n",
    "if trainer.state.best_model_checkpoint:  # 최적 체크포인트가 존재하는 경우\n",
    "    best_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "    trainer.save_model(best_checkpoint_path)\n",
    "    print(f\"Merged model saved to {best_checkpoint_path}\")\n",
    "else:\n",
    "    print(\"No best checkpoint found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}