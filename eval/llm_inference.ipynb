{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c150851-0586-4e5c-b114-782f9896fa24",
   "metadata": {},
   "source": [
    "## Fine-tuning 전 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer , pipeline\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import re\n",
    "import logging \n",
    "\n",
    "logging.getLogger(\"langchain\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52fbf9a",
   "metadata": {},
   "outputs": [],
   "source": "def question_answer(question, model_id):\n    from decouple import config\n    huggingface_token = config('HUGGINGFACE_TOKEN')\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)\n    tokenizer = AutoTokenizer.from_pretrained(model_id, huggingface_token=huggingface_token)\n\n    hf_pipeline = pipeline(\n        \"text-generation\", # 텍스트 생성\n        model=model,\n        tokenizer=tokenizer, \n        truncation=True,\n        max_length=512, # 생성할 최대 토큰 길이 지정 \n        temperature=0.5, # 생성의 창의성을 제어하는 매개변수\n        top_p=0.9,\n        repetition_penalty=1.1,\n        do_sample=True,\n        eos_token_id=tokenizer.eos_token_id,\n        device=0)\n    \n    llm = HuggingFacePipeline(pipeline=hf_pipeline)\n    \n    # 사용자 정의 프롬프트 템플릿 정의 \n    prompt_template = \"\"\"아래의 문맥을 사용하여 질문에 답하십시오. \n    만약 답을 모른다면, 모른다고 말하고 답을 지어내지 마십시오.\n\n    질문:{question}\n    답변:\"\"\"\n    \n    PROMPT = PromptTemplate(template=prompt_template, input_variables=['question'])\n    \n     # LLMChain 설정\n    llm_chain = LLMChain(\n        llm=llm,\n        prompt=PROMPT)\n    \n    response = llm_chain.run(question = question)\n    clean_response = response.split(\"답변:\")[-1].strip()\n    final_response = re.sub(r'</?div.*?>|</u>|</s>|</?[^>]+>|<pad>|<unk>|<mask>', '', clean_response)\n\n    print('질문:', question)\n    print('답변:', final_response)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9375b431",
   "metadata": {},
   "outputs": [],
   "source": "from decouple import config\n\nhuggingface_token = config('HUGGINGFACE_TOKEN')\n\nmodel_id = \"iljoo/iljoo-chatbot-8b-test\"\nquestion_answer('친환경 에너지를 활용하여 온실가스 배출을 줄이는 방법은 무엇인가요?',model_id)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c18c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}